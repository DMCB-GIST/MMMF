{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8cf25c4",
   "metadata": {},
   "source": [
    "### Import Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5626d86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from util import *\n",
    "from model import *\n",
    "import torch\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "025b1109",
   "metadata": {},
   "source": [
    "### Hyperparameter Setting\n",
    "\n",
    "<table style=\"width: 100%\">\n",
    "    <colgroup>\n",
    "       <col span=\"1\" style=\"width: 60%;\">\n",
    "       <col span=\"1\" style=\"width: 20%;\">\n",
    "       <col span=\"1\" style=\"width: 20%;\">\n",
    "    </colgroup>\n",
    "    <thead>\n",
    "        <tr>\n",
    "            <th>Description</th>\n",
    "            <th>Code</th>\n",
    "            <th>Value</th>\n",
    "        </tr>\n",
    "    </thead>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Learning rate ($\\eta_1$) for train multiple coefficient matrices ($V_{i,r}$)</td>\n",
    "            <td>re_lr</td>\n",
    "            <td>$1\\mathrm{e}{-3}$</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Regularization rate ($\\alpha$) for train multiple coefficient matrices ($V_{i,r}$)</td>\n",
    "            <td>re_reg</td>\n",
    "            <td>$1\\mathrm{e}{-4}$</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Learning rate ($\\eta_2$) for classifier ($f(\\cdot)$) and a common basis matrix ($U$)</td>\n",
    "            <td>clf_lr</td>\n",
    "            <td>$1\\mathrm{e}{-3}$</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>Regularization rate ($\\beta$) for classifier ($f(\\cdot)$) and a common basis matrix ($U$)</td>\n",
    "            <td>clf_reg</td>\n",
    "            <td>$1\\mathrm{e}{-3}$</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>The dimension of multiple coefficient matrices as $[D_{i2}, D_{i3}]$, and define the dimension of the hidden layer of the classifier as $h$</td>\n",
    "            <td>[du1, du2, du3]</td>\n",
    "            <td>$[110, 90, 70]$</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "    <tbody>\n",
    "        <tr>\n",
    "            <td>certain patience ($T_{\\text{p}}$)</td>\n",
    "            <td>patience</td>\n",
    "            <td>30</td>\n",
    "        </tr>\n",
    "    </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c75c25ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device Setting\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Hyperparameter Setting\n",
    "hyper_dict = {'patience': 30, 'epoch': 50000,\n",
    "              'device': device,\n",
    "              'du1': 110, 'du2': 90, 'du3': 70,\n",
    "              'clf_lr': 1e-3, 'clf_reg': 1e-4, 're_lr': 1e-3, 're_reg': 1e-3}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b2de05",
   "metadata": {},
   "source": [
    "### Example 3 Modality & Label Data\n",
    "- Modality 1: 1000 x 1000\n",
    "- Modality 2: 1000 x 2000\n",
    "- Modality 3: 1000 x 3000\n",
    "- Label: Binary Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7dd41ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Toy Example\n",
    "modality_1 = np.random.rand(1000, 1000)\n",
    "modality_2 = np.random.rand(1000, 2000)\n",
    "modality_3 = np.random.rand(1000, 3000)\n",
    "label = np.random.randint(2, size = 1000)\n",
    "\n",
    "# Split Toy Example Dataset\n",
    "index = np.arange(1000)\n",
    "train_index, val_index = train_test_split(index, test_size=0.2, random_state=3)\n",
    "train_index, test_index = train_test_split(train_index, test_size=0.25, random_state=3)\n",
    "index = [train_index, val_index, test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c30f1c",
   "metadata": {},
   "source": [
    "### Model Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbdd4bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Train with 3 Modality\n",
    "def train(modality_1, modality_2, modality_3, label, index, hyper_dict):\n",
    "    # Index\n",
    "    train_index, val_index, test_index = index\n",
    "    \n",
    "    # For Early Stopping\n",
    "    early_stopping = EarlyStopping(patience=300, delta=0)\n",
    "    best_loss = np.Inf\n",
    "    net_best_model_sd = None\n",
    "    clf_best_model_sd = None\n",
    "    \n",
    "    # Define Model\n",
    "    net = DMF(modality_1, modality_2, modality_3,\n",
    "              hyper_dict['du1'], hyper_dict['du2'], hyper_dict['device']).to(hyper_dict['device'])\n",
    "\n",
    "    clf = Softmax_Classifier(hyper_dict['du2'], hyper_dict['du3']).to(hyper_dict['device'])\n",
    "\n",
    "    # Optimizer\n",
    "    individual_param_list, common_param = net.get_param()\n",
    "    mf_individual_optimizer = torch.optim.Adam(individual_param_list, lr=hyper_dict['re_lr'], weight_decay=hyper_dict['re_reg'])\n",
    "    mf_common_optimizer = torch.optim.Adam(common_param, lr=hyper_dict['clf_lr'], weight_decay=hyper_dict['clf_reg'])\n",
    "    clf_optimizer = torch.optim.Adam(clf.parameters(), lr=hyper_dict['clf_lr'], weight_decay=hyper_dict['clf_reg'])\n",
    "\n",
    "    # Model Train // Validation\n",
    "    for i in range(hyper_dict['epoch']):\n",
    "        # For Train\n",
    "        net.train()\n",
    "        clf.train()\n",
    "\n",
    "        # Optimzer.zero_grad()\n",
    "        mf_individual_optimizer.zero_grad()\n",
    "        mf_common_optimizer.zero_grad()\n",
    "        clf_optimizer.zero_grad()\n",
    "\n",
    "        # Reconstruction\n",
    "        m1_loss, m2_loss, m3_loss = net(modality_1, modality_2, modality_3)\n",
    "\n",
    "        # Reconstruction G1 Grad of U\n",
    "        m1_loss.backward()\n",
    "        g1 = net.u.grad.clone()\n",
    "        net.u.grad.zero_()\n",
    "\n",
    "        # Reconstruction G2 Grad of U\n",
    "        m2_loss.backward()\n",
    "        g2 = net.u.grad.clone()\n",
    "        net.u.grad.zero_()\n",
    "\n",
    "        # Reconstruction G3 Grad of U\n",
    "        m3_loss.backward()\n",
    "        g3 = net.u.grad.clone()\n",
    "        net.u.grad.zero_()\n",
    "\n",
    "        # Classification CLF Grad of U\n",
    "        if i > hyper_dict['patience']:\n",
    "            # Over Sampling\n",
    "            smote = SMOTE(random_state=0)\n",
    "            u_over, y_train_over = smote.fit_resample(net.u[train_index].cpu().detach().numpy(), y_train)\n",
    "\n",
    "            u_over = torch.tensor(u_over, requires_grad=True).float()\n",
    "            u_over.retain_grad()\n",
    "\n",
    "            # Softmax Classifier\n",
    "            clf_loss = clf(u_over.to(hyper_dict['device']),\n",
    "                           torch.tensor(y_train_over).to(hyper_dict['device'])).to(hyper_dict['device'])\n",
    "            clf_loss.backward()\n",
    "            \n",
    "            # Indexing for backward\n",
    "            g4 = torch.zeros_like(net.u.grad).to(hyper_dict['device'])\n",
    "            g4[train_index] = u_over.grad[:len(train_index)].clone().to(hyper_dict['device'])\n",
    "            net.u.grad.zero_()\n",
    "\n",
    "            # Projection Grad => U[train_index] -> g1, g2, g3, g4, U[~train_index] -> g1, g2, g3\n",
    "            proj_grad = project_conflicting([g1.flatten(), g2.flatten(), g3.flatten()], g4.flatten())\n",
    "            proj_grad = proj_grad.reshape_as(net.u.grad)\n",
    "            net.u.grad = proj_grad\n",
    "\n",
    "            # optimizer.step()\n",
    "            mf_individual_optimizer.step()\n",
    "            mf_common_optimizer.step()\n",
    "            clf_optimizer.step()\n",
    "\n",
    "        else:\n",
    "            mean_grad = (g1 + g2 + g3) / 3\n",
    "            net.u.grad = mean_grad\n",
    "            # optimizer.step()\n",
    "            mf_individual_optimizer.step()\n",
    "            mf_common_optimizer.step()\n",
    "        \n",
    "        # Check Train Performance\n",
    "        if i % 100 == 0:\n",
    "            prob, prediction = clf.predict(net.u[train_index])\n",
    "            prob = prob.detach().cpu().numpy()\n",
    "            prediction = prediction.detach().cpu().numpy()\n",
    "            y_train = label[train_index]\n",
    "            ba, f1, auc, mcc = calculate_metric(y_train, prediction, prob)\n",
    "            if i == 0:\n",
    "                print('Training: Epoch [{}/{}]\\n\\\n",
    "                M1 Reconstruction Loss: {:.4f}, M2 Reconstruction Loss: {:.4f}, M3 Reconstruction Loss: {:.4f}'.format(\n",
    "                    i, hyper_dict['epoch'], m1_loss.item(), m2_loss.item(), m3_loss.item()))\n",
    "            else:\n",
    "                print('Training: Epoch [{}/{}]\\n\\\n",
    "                M1 Reconstruction Loss: {:.4f}, M2 Reconstruction Loss: {:.4f}, M3 Reconstruction Loss: {:.4f}\\n\\\n",
    "                Classification Loss: {:.4f}'.format(i, hyper_dict['epoch'], m1_loss.item(), m2_loss.item(), m3_loss.item(), clf_loss.item()))\n",
    "                print('Ba: {:.4f}, F1: {:.4f}, AUC: {:.4f} MCC: {:.4f}'.format(ba, f1, auc, mcc))\n",
    "\n",
    "        # Model Validation\n",
    "        net.eval()\n",
    "        clf.eval()\n",
    "        \n",
    "        m1_loss, m2_loss, m3_loss = net(modality_1, modality_2, modality_3)\n",
    "        clf_loss = clf(net.u[val_index], torch.tensor(label[val_index]).to(device))\n",
    "\n",
    "        # Check Early Stopping\n",
    "        early_stopping(clf_loss)\n",
    "        if clf_loss < best_loss:\n",
    "            net_best_model_sd = copy.deepcopy(net.state_dict())\n",
    "            clf_best_model_sd = copy.deepcopy(clf.state_dict())\n",
    "            best_loss = clf_loss\n",
    "\n",
    "        # Early Stopping\n",
    "        if early_stopping.early_stop:\n",
    "            print('Early Stopping... Epoch [{}/{}]'.format(i, hyper_dict['epoch']))\n",
    "            print('Best CLassification Loss: {:.4f}'.format(best_loss))\n",
    "            break\n",
    "        \n",
    "        # Validation\n",
    "        if i % 100 == 0:\n",
    "            # Check Validation Performance\n",
    "            prob, prediction = clf.predict(net.u[val_index])\n",
    "            prob = prob.detach().cpu().numpy()\n",
    "            prediction = prediction.detach().cpu().numpy()\n",
    "            y_val = label[val_index]\n",
    "            ba, f1, auc, mcc = calculate_metric(y_val, prediction, prob)\n",
    "            if i == 0:\n",
    "                print('Validation: Epoch [{}/{}]\\n\\\n",
    "                M1 Reconstruction Loss: {:.4f}, M2 Reconstruction Loss: {:.4f}, M3 Reconstruction Loss: {:.4f}\\n'.format(\n",
    "                    i, hyper_dict['epoch'], m1_loss.item(), m2_loss.item(), m3_loss.item()))\n",
    "            else:\n",
    "                print('Validation: Epoch [{}/{}]\\n\\\n",
    "                M1 Reconstruction Loss: {:.4f}, M2 Reconstruction Loss: {:.4f}, M3 Reconstruction Loss: {:.4f}\\n\\\n",
    "                Classification Loss: {:.4f}'.format(\n",
    "                i, hyper_dict['epoch'], m1_loss.item(), m2_loss.item(), m3_loss.item(), clf_loss.item()))\n",
    "                print('Ba: {:.4f}, F1: {:.4f}, AUC: {:.4f} MCC: {:.4f}\\n'.format(ba, f1, auc, mcc))\n",
    "    \n",
    "    # Test Model Performance\n",
    "    net.load_state_dict(net_best_model_sd)\n",
    "    net.eval()\n",
    "    clf.load_state_dict(clf_best_model_sd)\n",
    "    clf.eval()\n",
    "    \n",
    "    # Test\n",
    "    prob, prediction = clf.predict(net.u[test_index])\n",
    "    prob = prob.detach().cpu().numpy()\n",
    "    prediction = prediction.detach().cpu().numpy()\n",
    "    ba, f1, auc, mcc = calculate_metric(label[test_index], prediction, prob)\n",
    "    print('\\nTest Performace\\nBa: {:.4f}, F1: {:.4f}, AUC: {:.4f} MCC: {:.4f}'.format(ba, f1, auc, mcc))\n",
    "\n",
    "    return net, clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3851319f",
   "metadata": {},
   "source": [
    "### Example Code Train & Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43acdcc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: Epoch [0/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "Validation: Epoch [0/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "\n",
      "Training: Epoch [100/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "                Classification Loss: 0.3169\n",
      "Ba: 1.0000, F1: 1.0000, AUC: 1.0000 MCC: 1.0000\n",
      "Validation: Epoch [100/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "                Classification Loss: 0.6918\n",
      "Ba: 0.4814, F1: 0.7063, AUC: 0.5054 MCC: -0.0720\n",
      "\n",
      "Training: Epoch [200/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "                Classification Loss: 0.3167\n",
      "Ba: 1.0000, F1: 1.0000, AUC: 1.0000 MCC: 1.0000\n",
      "Validation: Epoch [200/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "                Classification Loss: 0.6941\n",
      "Ba: 0.5000, F1: 0.0000, AUC: 0.5212 MCC: 0.0000\n",
      "\n",
      "Training: Epoch [300/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "                Classification Loss: 0.3156\n",
      "Ba: 1.0000, F1: 1.0000, AUC: 1.0000 MCC: 1.0000\n",
      "Validation: Epoch [300/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "                Classification Loss: 0.6937\n",
      "Ba: 0.5000, F1: 0.0000, AUC: 0.4120 MCC: 0.0000\n",
      "\n",
      "Training: Epoch [400/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "                Classification Loss: 0.3151\n",
      "Ba: 1.0000, F1: 1.0000, AUC: 1.0000 MCC: 1.0000\n",
      "Validation: Epoch [400/50000]\n",
      "                M1 Reconstruction Loss: 0.3333, M2 Reconstruction Loss: 0.3337, M3 Reconstruction Loss: 0.3337\n",
      "                Classification Loss: 0.6932\n",
      "Ba: 0.5000, F1: 0.0000, AUC: 0.5015 MCC: 0.0000\n",
      "\n",
      "Early Stopping... Epoch [435/50000]\n",
      "Best CLassification Loss: 0.6907\n",
      "\n",
      "Test Performace\n",
      "Ba: 0.5000, F1: 0.6885, AUC: 0.5229 MCC: 0.0000\n"
     ]
    }
   ],
   "source": [
    "net, clf = train(modality_1, modality_2, modality_3, label, index, hyper_dict)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
